{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bpe_tokenizer import BPETokenizer\n",
    "import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "import torchmetrics\n",
    "import torchinfo\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"TA-EN NMT\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_bpe = None\n",
    "english_bpe = None\n",
    "with open(\"trg_bpe1.vocab.json\",\"r\") as f:\n",
    "    english_bpe = json.load(f)\n",
    "with open(\"src_bpe1.vocab.json\",\"r\") as f:\n",
    "    tamil_bpe = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21401, 13465)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tamil_bpe), len(english_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, datasets, src_bpe_path, trg_bpe_path):\n",
    "        '''\n",
    "        datasets: list[str] -> list of dataset csv files\n",
    "        src_bpe_path: str -> path to source bpe file (path/to/src_bpe1.vocab.json do not include .vocab.json)\n",
    "        trg_bpe_path: str -> path to target bpe file (path/to/trg_bpe1.vocab.json do not include .vocab.json)\n",
    "        '''\n",
    "        initial_csv = pd.read_csv(datasets[0])\n",
    "        for dataset in datasets[1:]:\n",
    "            initial_csv = pd.concat([initial_csv, pd.read_csv(dataset)])\n",
    "        self.BPE_tokenizer_ta = BPETokenizer.load(src_bpe_path, \"ta\")\n",
    "        self.BPE_tokenizer_en = BPETokenizer.load(trg_bpe_path, \"en\")\n",
    "        self.tamil = initial_csv[\"ta\"].tolist()\n",
    "        self.english = initial_csv[\"en\"].tolist()\n",
    "        assert len(self.tamil) == len(self.english), \"Tamil and English sentences are not of the same length\"\n",
    "    def __len__(self): return len(self.tamil)\n",
    "    def __getitem__(self, idx):\n",
    "        tamil_sentence = self.tamil[idx]\n",
    "        english_sentence = self.english[idx]\n",
    "        tamil_tokens = self.BPE_tokenizer_ta.encode(tamil_sentence)\n",
    "        english_tokens = self.BPE_tokenizer_en.encode(english_sentence)\n",
    "        \n",
    "        trg_pad_mask = None\n",
    "        src_pad_mask = None\n",
    "        if len(english_tokens) > len(tamil_tokens):\n",
    "            pad_len = len(english_tokens) - len(tamil_tokens)\n",
    "            src_pad_mask = torch.zeros(pad_len)\n",
    "            tamil_tokens.extend([0] * pad_len)\n",
    "            tamil_tokens = torch.tensor(tamil_tokens)\n",
    "            src_pad_mask = src_pad_mask == tamil_tokens\n",
    "        elif len(tamil_tokens) > len(english_tokens):\n",
    "            pad_len = len(tamil_tokens) - len(english_tokens)\n",
    "            trg_pad_mask = torch.zeros(pad_len)\n",
    "            english_tokens.extend([0] * pad_len)\n",
    "            english_tokens = torch.tensor(english_tokens)\n",
    "            trg_pad_mask = trg_pad_mask == english_tokens\n",
    "        return tamil_tokens, english_tokens, src_pad_mask, trg_pad_mask\n",
    "\n",
    "dataset = TranslationDataset([\"en-ta//pmindia.v1.ta-en 39k.csv\",\"en-ta//general_en_ta 87k.csv\"], \"src_bpe1\", \"trg_bpe1\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\miniconda3\\envs\\xformers-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_heads=8, n_layers=6, d_ff=2048, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads, d_ff, dropout),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, n_heads, d_ff, dropout),\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = nn.Parameter(torch.zeros(10000, d_model))  # Positional encoding\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer()\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.src_emb(src) + self.pos_enc[:src.size(0)]\n",
    "        tgt = self.tgt_emb(tgt) + self.pos_enc[:tgt.size(0)]\n",
    "        memory = self.encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.decoder(tgt, memory, memory_key_padding_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Example usage (later in training)\n",
    "model = Transformer(src_vocab_size=21401, tgt_vocab_size=13465)  # Adjust vocab sizes\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()  # Use RTX 4060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n",
    "        self.scaler = GradScaler()\n",
    "        self.metrics = {\n",
    "            'loss': [],\n",
    "            'bleu': []\n",
    "        }\n",
    "\n",
    "    def set_optimizer(self, optimizer_name, **kwargs):\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), **kwargs)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for tamil_batch, english_batch in self.train_loader:\n",
    "            tamil = torch.tensor(tamil_batch, dtype=torch.long).to(self.device)\n",
    "            english = torch.tensor(english_batch, dtype=torch.long).to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = self.model(tamil, english[:, :-1])\n",
    "                loss = self.criterion(output.view(-1, output.size(-1)), english[:, 1:].contiguous().view(-1))\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        self.metrics['loss'].append(avg_loss)\n",
    "        logger.info(f\"Training Loss: {avg_loss}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for tamil_batch, english_batch in self.val_loader:\n",
    "                tamil = torch.tensor(tamil_batch, dtype=torch.long).to(self.device)\n",
    "                english = torch.tensor(english_batch, dtype=torch.long).to(self.device)\n",
    "                output = self.model(tamil, english[:, :-1])\n",
    "                loss = self.criterion(output.view(-1, output.size(-1)), english[:, 1:].contiguous().view(-1))\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        self.metrics['loss'].append(avg_loss)\n",
    "        logger.info(f\"Validation Loss: {avg_loss}\")\n",
    "\n",
    "    def calculate_bleu(self, references, hypotheses):\n",
    "        # Placeholder for BLEU score calculation\n",
    "        bleu_score = 0.0\n",
    "        self.metrics['bleu'].append(bleu_score)\n",
    "        logger.info(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "            logger.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            self.train_epoch()\n",
    "            self.evaluate()\n",
    "            # Assuming you have a method to get references and hypotheses\n",
    "            # references, hypotheses = self.get_references_and_hypotheses()\n",
    "            # self.calculate_bleu(references, hypotheses)\n",
    "\n",
    "    def update_parameters(self, new_params):\n",
    "        for param, value in new_params.items():\n",
    "            if hasattr(self.model, param):\n",
    "                setattr(self.model, param, value)\n",
    "            else:\n",
    "                logger.warning(f\"Parameter {param} not found in model.\")\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = TranslationDataset([\"en-ta//pmindia.v1.ta-en 39k.csv\", \"en-ta//general_en_ta 87k.csv\"], \"src_bpe1\", \"trg_bpe1\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = Transformer(src_vocab_size=21401, tgt_vocab_size=13465).to(device)\n",
    "trainer = NMTTrainer(model, loader, loader, device)  # Assuming same loader for train/val for simplicity\n",
    "trainer.set_optimizer('adam', lr=0.0001)\n",
    "trainer.train(num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
